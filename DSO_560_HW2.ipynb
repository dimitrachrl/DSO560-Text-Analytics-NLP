{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCb3H08q0t9L",
        "outputId": "a6a2dcc3-a49e-4dc1-e80f-75a3142be5ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the Romeo & Juliet text.\n",
        "# Retrieve only the first 1,000 characters from the text #\n",
        "textTotal = open('RomeoJuliet.txt').read()\n",
        "blobTotal = TextBlob(textTotal) \n",
        "######################################\n",
        "numChars = 1000\n",
        "text1000 = textTotal[0:numChars+1]\n",
        "blob1000 = TextBlob(text1000)"
      ],
      "metadata": {
        "id": "ZHH08Z1P4jHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_words(list_words, numElementsRow):\n",
        " for j in range (0, len(list_words), numElementsRow):\n",
        "   for i in range(j, j+numElementsRow):\n",
        "     if (i >= len(list_words)):\n",
        "       break\n",
        "     print(i, '.', list_words[i], end=', ')\n",
        "   print()\n",
        " print()"
      ],
      "metadata": {
        "id": "52j-Iopi9tRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Tokenization**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eAJc7r1s5s5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Find all the word tokens using regular expressions in text1000 string variable."
      ],
      "metadata": {
        "id": "cFgmKcza6RnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "wordToken = r\"\\w+\" #Find all words\n",
        "\n",
        "matches = re.findall(wordToken, text1000)\n",
        "\n",
        "for match in matches:\n",
        "  print(match)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOVMZlnI6Uzk",
        "outputId": "40450a47-a7d2-4eaa-8c82-58053e786bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project\n",
            "Gutenberg\n",
            "s\n",
            "Romeo\n",
            "and\n",
            "Juliet\n",
            "by\n",
            "William\n",
            "Shakespeare\n",
            "This\n",
            "eBook\n",
            "is\n",
            "for\n",
            "the\n",
            "use\n",
            "of\n",
            "anyone\n",
            "anywhere\n",
            "in\n",
            "the\n",
            "United\n",
            "States\n",
            "and\n",
            "most\n",
            "other\n",
            "parts\n",
            "of\n",
            "the\n",
            "world\n",
            "at\n",
            "no\n",
            "cost\n",
            "and\n",
            "with\n",
            "almost\n",
            "no\n",
            "restrictions\n",
            "whatsoever\n",
            "You\n",
            "may\n",
            "copy\n",
            "it\n",
            "give\n",
            "it\n",
            "away\n",
            "or\n",
            "re\n",
            "use\n",
            "it\n",
            "under\n",
            "the\n",
            "terms\n",
            "of\n",
            "the\n",
            "Project\n",
            "Gutenberg\n",
            "License\n",
            "included\n",
            "with\n",
            "this\n",
            "eBook\n",
            "or\n",
            "online\n",
            "at\n",
            "www\n",
            "gutenberg\n",
            "org\n",
            "If\n",
            "you\n",
            "are\n",
            "not\n",
            "located\n",
            "in\n",
            "the\n",
            "United\n",
            "States\n",
            "you\n",
            "ll\n",
            "have\n",
            "to\n",
            "check\n",
            "the\n",
            "laws\n",
            "of\n",
            "the\n",
            "country\n",
            "where\n",
            "you\n",
            "are\n",
            "located\n",
            "before\n",
            "using\n",
            "this\n",
            "ebook\n",
            "Title\n",
            "Romeo\n",
            "and\n",
            "Juliet\n",
            "Author\n",
            "William\n",
            "Shakespeare\n",
            "Release\n",
            "Date\n",
            "November\n",
            "1998\n",
            "Etext\n",
            "1513\n",
            "Last\n",
            "Updated\n",
            "January\n",
            "30\n",
            "2019\n",
            "Language\n",
            "English\n",
            "Character\n",
            "set\n",
            "encoding\n",
            "UTF\n",
            "8\n",
            "START\n",
            "OF\n",
            "THIS\n",
            "PROJECT\n",
            "GUTENBERG\n",
            "EBOOK\n",
            "ROMEO\n",
            "AND\n",
            "JULIET\n",
            "This\n",
            "etext\n",
            "was\n",
            "produced\n",
            "by\n",
            "the\n",
            "PG\n",
            "Shakespeare\n",
            "Team\n",
            "a\n",
            "team\n",
            "of\n",
            "about\n",
            "twenty\n",
            "Project\n",
            "Gutenberg\n",
            "volunteers\n",
            "THE\n",
            "TRAGEDY\n",
            "OF\n",
            "ROMEO\n",
            "AND\n",
            "JULIET\n",
            "by\n",
            "William\n",
            "Shakespeare\n",
            "Contents\n",
            "THE\n",
            "PROLOGUE\n",
            "ACT\n",
            "I\n",
            "Scene\n",
            "I\n",
            "A\n",
            "public\n",
            "place\n",
            "Scene\n",
            "II\n",
            "A\n",
            "Street\n",
            "Sc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Find all the sentence tokens using regular expressions in text1000 string variable."
      ],
      "metadata": {
        "id": "YfB1b-2B64BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_ending = re.compile(r\"[.?!]\")\n",
        "\n",
        "matches = sentence_ending.finditer(text1000)\n",
        "for match in matches:\n",
        "  print(match)\n",
        "#11 Sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ncRHYr87nW2",
        "outputId": "b5e56f56-4968-41a1-d487-de1393264df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(216, 217), match='.'>\n",
            "<re.Match object; span=(351, 352), match='.'>\n",
            "<re.Match object; span=(361, 362), match='.'>\n",
            "<re.Match object; span=(365, 366), match='.'>\n",
            "<re.Match object; span=(502, 503), match='.'>\n",
            "<re.Match object; span=(855, 856), match='.'>\n",
            "<re.Match object; span=(945, 946), match='.'>\n",
            "<re.Match object; span=(961, 962), match='.'>\n",
            "<re.Match object; span=(977, 978), match='.'>\n",
            "<re.Match object; span=(987, 988), match='.'>\n",
            "<re.Match object; span=(997, 998), match='.'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(re.split(sentence_ending, text1000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nfj0cWRH68TS",
        "outputId": "b114cac3-8ede-43e5-90e5-3bdfbe19f314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\nProject Gutenberg’s Romeo and Juliet, by William Shakespeare\\n\\nThis eBook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever', ' You may copy it, give it away or re-use it under the terms\\nof the Project Gutenberg License included with this eBook or online at\\nwww', 'gutenberg', 'org', ' If you are not located in the United States, you’ll\\nhave to check the laws of the country where you are located before using\\nthis ebook', '\\n\\n\\n\\nTitle: Romeo and Juliet\\n\\nAuthor: William Shakespeare\\n\\nRelease Date: November, 1998 [Etext #1513]\\nLast Updated: January 30, 2019\\n\\nLanguage: English\\n\\nCharacter set encoding: UTF-8\\n\\n*** START OF THIS PROJECT GUTENBERG EBOOK ROMEO AND JULIET ***\\n\\n\\n\\nThis etext was produced by the PG Shakespeare Team,\\na team of about twenty Project Gutenberg volunteers', '\\n\\n\\nTHE TRAGEDY OF ROMEO AND JULIET\\n\\n\\n\\nby William Shakespeare\\n\\n\\n\\n\\n\\n\\nContents\\n\\nTHE PROLOGUE', '\\n\\nACT I\\nScene I', ' A public place', '\\nScene II', ' A Street', '\\nSc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Find all the word tokens using NLTK library in text1000 string variable."
      ],
      "metadata": {
        "id": "DLtZXgH67wcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaUHqUqD70lc",
        "outputId": "c80aac5e-7cb5-4800-90f5-4b3e8ea95ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "0PanCfsO8Ap6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(text1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WQTosmj8Lms",
        "outputId": "931b42db-cd42-4459-8b7e-7d189b6efdc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Project',\n",
              " 'Gutenberg',\n",
              " '’',\n",
              " 's',\n",
              " 'Romeo',\n",
              " 'and',\n",
              " 'Juliet',\n",
              " ',',\n",
              " 'by',\n",
              " 'William',\n",
              " 'Shakespeare',\n",
              " 'This',\n",
              " 'eBook',\n",
              " 'is',\n",
              " 'for',\n",
              " 'the',\n",
              " 'use',\n",
              " 'of',\n",
              " 'anyone',\n",
              " 'anywhere',\n",
              " 'in',\n",
              " 'the',\n",
              " 'United',\n",
              " 'States',\n",
              " 'and',\n",
              " 'most',\n",
              " 'other',\n",
              " 'parts',\n",
              " 'of',\n",
              " 'the',\n",
              " 'world',\n",
              " 'at',\n",
              " 'no',\n",
              " 'cost',\n",
              " 'and',\n",
              " 'with',\n",
              " 'almost',\n",
              " 'no',\n",
              " 'restrictions',\n",
              " 'whatsoever',\n",
              " '.',\n",
              " 'You',\n",
              " 'may',\n",
              " 'copy',\n",
              " 'it',\n",
              " ',',\n",
              " 'give',\n",
              " 'it',\n",
              " 'away',\n",
              " 'or',\n",
              " 're-use',\n",
              " 'it',\n",
              " 'under',\n",
              " 'the',\n",
              " 'terms',\n",
              " 'of',\n",
              " 'the',\n",
              " 'Project',\n",
              " 'Gutenberg',\n",
              " 'License',\n",
              " 'included',\n",
              " 'with',\n",
              " 'this',\n",
              " 'eBook',\n",
              " 'or',\n",
              " 'online',\n",
              " 'at',\n",
              " 'www.gutenberg.org',\n",
              " '.',\n",
              " 'If',\n",
              " 'you',\n",
              " 'are',\n",
              " 'not',\n",
              " 'located',\n",
              " 'in',\n",
              " 'the',\n",
              " 'United',\n",
              " 'States',\n",
              " ',',\n",
              " 'you',\n",
              " '’',\n",
              " 'll',\n",
              " 'have',\n",
              " 'to',\n",
              " 'check',\n",
              " 'the',\n",
              " 'laws',\n",
              " 'of',\n",
              " 'the',\n",
              " 'country',\n",
              " 'where',\n",
              " 'you',\n",
              " 'are',\n",
              " 'located',\n",
              " 'before',\n",
              " 'using',\n",
              " 'this',\n",
              " 'ebook',\n",
              " '.',\n",
              " 'Title',\n",
              " ':',\n",
              " 'Romeo',\n",
              " 'and',\n",
              " 'Juliet',\n",
              " 'Author',\n",
              " ':',\n",
              " 'William',\n",
              " 'Shakespeare',\n",
              " 'Release',\n",
              " 'Date',\n",
              " ':',\n",
              " 'November',\n",
              " ',',\n",
              " '1998',\n",
              " '[',\n",
              " 'Etext',\n",
              " '#',\n",
              " '1513',\n",
              " ']',\n",
              " 'Last',\n",
              " 'Updated',\n",
              " ':',\n",
              " 'January',\n",
              " '30',\n",
              " ',',\n",
              " '2019',\n",
              " 'Language',\n",
              " ':',\n",
              " 'English',\n",
              " 'Character',\n",
              " 'set',\n",
              " 'encoding',\n",
              " ':',\n",
              " 'UTF-8',\n",
              " '*',\n",
              " '*',\n",
              " '*',\n",
              " 'START',\n",
              " 'OF',\n",
              " 'THIS',\n",
              " 'PROJECT',\n",
              " 'GUTENBERG',\n",
              " 'EBOOK',\n",
              " 'ROMEO',\n",
              " 'AND',\n",
              " 'JULIET',\n",
              " '*',\n",
              " '*',\n",
              " '*',\n",
              " 'This',\n",
              " 'etext',\n",
              " 'was',\n",
              " 'produced',\n",
              " 'by',\n",
              " 'the',\n",
              " 'PG',\n",
              " 'Shakespeare',\n",
              " 'Team',\n",
              " ',',\n",
              " 'a',\n",
              " 'team',\n",
              " 'of',\n",
              " 'about',\n",
              " 'twenty',\n",
              " 'Project',\n",
              " 'Gutenberg',\n",
              " 'volunteers',\n",
              " '.',\n",
              " 'THE',\n",
              " 'TRAGEDY',\n",
              " 'OF',\n",
              " 'ROMEO',\n",
              " 'AND',\n",
              " 'JULIET',\n",
              " 'by',\n",
              " 'William',\n",
              " 'Shakespeare',\n",
              " 'Contents',\n",
              " 'THE',\n",
              " 'PROLOGUE',\n",
              " '.',\n",
              " 'ACT',\n",
              " 'I',\n",
              " 'Scene',\n",
              " 'I',\n",
              " '.',\n",
              " 'A',\n",
              " 'public',\n",
              " 'place',\n",
              " '.',\n",
              " 'Scene',\n",
              " 'II',\n",
              " '.',\n",
              " 'A',\n",
              " 'Street',\n",
              " '.',\n",
              " 'Sc']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Find all the sentence tokens using NLTK library in text1000 string variable."
      ],
      "metadata": {
        "id": "uFUtbBfP8dg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(text1000)\n",
        "print_words(sentences, 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Eb-27Hk8e4_",
        "outputId": "f2c4cb08-f1a0-443d-d68c-780d65228348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 . \n",
            "Project Gutenberg’s Romeo and Juliet, by William Shakespeare\n",
            "\n",
            "This eBook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever., \n",
            "1 . You may copy it, give it away or re-use it under the terms\n",
            "of the Project Gutenberg License included with this eBook or online at\n",
            "www.gutenberg.org., \n",
            "2 . If you are not located in the United States, you’ll\n",
            "have to check the laws of the country where you are located before using\n",
            "this ebook., \n",
            "3 . Title: Romeo and Juliet\n",
            "\n",
            "Author: William Shakespeare\n",
            "\n",
            "Release Date: November, 1998 [Etext #1513]\n",
            "Last Updated: January 30, 2019\n",
            "\n",
            "Language: English\n",
            "\n",
            "Character set encoding: UTF-8\n",
            "\n",
            "*** START OF THIS PROJECT GUTENBERG EBOOK ROMEO AND JULIET ***\n",
            "\n",
            "\n",
            "\n",
            "This etext was produced by the PG Shakespeare Team,\n",
            "a team of about twenty Project Gutenberg volunteers., \n",
            "4 . THE TRAGEDY OF ROMEO AND JULIET\n",
            "\n",
            "\n",
            "\n",
            "by William Shakespeare\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Contents\n",
            "\n",
            "THE PROLOGUE., \n",
            "5 . ACT I\n",
            "Scene I., \n",
            "6 . A public place., \n",
            "7 . Scene II., \n",
            "8 . A Street., \n",
            "9 . Sc, \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis of Words**"
      ],
      "metadata": {
        "id": "F3y5mwlF9bCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Count and display the words in the first 1,000 characters of the text. Display all the words by"
      ],
      "metadata": {
        "id": "z9Oz453x9eK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_words(blob1000.words,10)\n",
        "print('Count of words = ', len(blob1000.words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l8k-vHBBOvI",
        "outputId": "af34e989-73df-4991-b0be-f4fa0a69b5ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 . Project, 1 . Gutenberg, 2 . ’, 3 . s, 4 . Romeo, 5 . and, 6 . Juliet, 7 . by, 8 . William, 9 . Shakespeare, \n",
            "10 . This, 11 . eBook, 12 . is, 13 . for, 14 . the, 15 . use, 16 . of, 17 . anyone, 18 . anywhere, 19 . in, \n",
            "20 . the, 21 . United, 22 . States, 23 . and, 24 . most, 25 . other, 26 . parts, 27 . of, 28 . the, 29 . world, \n",
            "30 . at, 31 . no, 32 . cost, 33 . and, 34 . with, 35 . almost, 36 . no, 37 . restrictions, 38 . whatsoever, 39 . You, \n",
            "40 . may, 41 . copy, 42 . it, 43 . give, 44 . it, 45 . away, 46 . or, 47 . re-use, 48 . it, 49 . under, \n",
            "50 . the, 51 . terms, 52 . of, 53 . the, 54 . Project, 55 . Gutenberg, 56 . License, 57 . included, 58 . with, 59 . this, \n",
            "60 . eBook, 61 . or, 62 . online, 63 . at, 64 . www.gutenberg.org, 65 . If, 66 . you, 67 . are, 68 . not, 69 . located, \n",
            "70 . in, 71 . the, 72 . United, 73 . States, 74 . you, 75 . ’, 76 . ll, 77 . have, 78 . to, 79 . check, \n",
            "80 . the, 81 . laws, 82 . of, 83 . the, 84 . country, 85 . where, 86 . you, 87 . are, 88 . located, 89 . before, \n",
            "90 . using, 91 . this, 92 . ebook, 93 . Title, 94 . Romeo, 95 . and, 96 . Juliet, 97 . Author, 98 . William, 99 . Shakespeare, \n",
            "100 . Release, 101 . Date, 102 . November, 103 . 1998, 104 . Etext, 105 . 1513, 106 . Last, 107 . Updated, 108 . January, 109 . 30, \n",
            "110 . 2019, 111 . Language, 112 . English, 113 . Character, 114 . set, 115 . encoding, 116 . UTF-8, 117 . START, 118 . OF, 119 . THIS, \n",
            "120 . PROJECT, 121 . GUTENBERG, 122 . EBOOK, 123 . ROMEO, 124 . AND, 125 . JULIET, 126 . This, 127 . etext, 128 . was, 129 . produced, \n",
            "130 . by, 131 . the, 132 . PG, 133 . Shakespeare, 134 . Team, 135 . a, 136 . team, 137 . of, 138 . about, 139 . twenty, \n",
            "140 . Project, 141 . Gutenberg, 142 . volunteers, 143 . THE, 144 . TRAGEDY, 145 . OF, 146 . ROMEO, 147 . AND, 148 . JULIET, 149 . by, \n",
            "150 . William, 151 . Shakespeare, 152 . Contents, 153 . THE, 154 . PROLOGUE, 155 . ACT, 156 . I, 157 . Scene, 158 . I, 159 . A, \n",
            "160 . public, 161 . place, 162 . Scene, 163 . II, 164 . A, 165 . Street, 166 . Sc, \n",
            "\n",
            "Count of words =  167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Count the words in the entire text."
      ],
      "metadata": {
        "id": "Ctq7FS7dBV80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Count of words = ', len(blobTotal.words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTFZRPX2BY_W",
        "outputId": "1358c94e-9023-44ca-81bb-1766dcb47db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of words =  30796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Count the unique words in the entire text."
      ],
      "metadata": {
        "id": "D3CyflfQBiHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_UniqueWord_Freq = blobTotal.word_counts.items()\n",
        "print('Total number of unique words in the text = ', len(dict_UniqueWord_Freq))\n",
        "print('Total number of words in the text = ', len(blobTotal.words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS-wXh9oBi4x",
        "outputId": "3eef033e-3e93-4845-c3c6-00c364766834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique words in the text =  4145\n",
            "Total number of words in the text =  30796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Count the unique words in the entire text after removing the stop-words from the list."
      ],
      "metadata": {
        "id": "vxL-aI-HB-9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_UniqueWord_No_Stop_Freq = [ ]\n",
        "stop_words = stopwords.words('english')\n",
        "for item in dict_UniqueWord_Freq:\n",
        "  if item[0] not in stop_words:\n",
        "    list_UniqueWord_No_Stop_Freq.append(item)\n",
        "print()\n",
        "\n",
        "print('Total number of unique words in the text =',len(dict_UniqueWord_Freq))\n",
        "print('Total number of unique words in the text AFTER removing stop words =',len(list_UniqueWord_No_Stop_Freq))\n",
        "print('Total number of stop words in the text =', len(dict_UniqueWord_Freq) - len(list_UniqueWord_No_Stop_Freq))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X4LnvuuCCwM",
        "outputId": "14347c44-3204-4197-c655-46d0c4d7d0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total number of unique words in the text = 4145\n",
            "Total number of unique words in the text AFTER removing stop words = 4017\n",
            "Total number of stop words in the text = 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Print the top-10 words in the entire text with highest frequency. Also display words’ frequency."
      ],
      "metadata": {
        "id": "alsSuoWeC-mE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying most frequently used words\n",
        "from operator import itemgetter\n",
        "sorted_items = sorted(list(dict_UniqueWord_Freq), key = itemgetter(1), reverse = True)\n",
        "print(type(sorted_items))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRzKLCo-Duxi",
        "outputId": "8f2da95f-2c21-462f-a57e-b639405ab1b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pulling out the top 10 entries\n",
        "numberTopWords = 10\n",
        "top10Words = sorted_items[0:numberTopWords]\n",
        "\n",
        "#Displaying using pandas\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(top10Words, columns = ['word','frequency'])\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C2O9LNaDCPL",
        "outputId": "b3098dd9-3231-4159-b7cc-2b0cd5ad86d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   word  frequency\n",
            "0   the        876\n",
            "1     ’        869\n",
            "2   and        808\n",
            "3     i        655\n",
            "4    to        626\n",
            "5     a        542\n",
            "6    of        519\n",
            "7    in        395\n",
            "8    is        372\n",
            "9  that        369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Print the top-10 words in the entire text with highest frequency after removing the stop-words from the list. Also display words’ frequency."
      ],
      "metadata": {
        "id": "ubVuwSn8EFKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sort the list using the frequency\n",
        "sorted_items = sorted(list_UniqueWord_No_Stop_Freq, key = itemgetter(1), reverse = True)"
      ],
      "metadata": {
        "id": "QHFX0rD4EKiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top10Words = sorted_items[0:numberTopWords]\n",
        "df = pd.DataFrame(top10Words, columns=['word','frequency'])\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlBr_-CmEczu",
        "outputId": "22521c99-16e2-425e-e527-7eb760c7d4d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      word  frequency\n",
            "0        ’        869\n",
            "1    romeo        320\n",
            "2     thou        278\n",
            "3   juliet        195\n",
            "4      thy        170\n",
            "5  capulet        163\n",
            "6    nurse        149\n",
            "7     love        148\n",
            "8     thee        138\n",
            "9     lady        117\n"
          ]
        }
      ]
    }
  ]
}